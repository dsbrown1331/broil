{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import mdp\n",
    "import mdp_worlds\n",
    "import utils\n",
    "import numpy as np\n",
    "import random \n",
    "import bayesian_irl\n",
    "import plot_gridworld as pg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#used this to plot the diagrams in the paper currently.\n",
    "\n",
    "init_seed = 1#4 + 10\n",
    "np.random.seed(init_seed)\n",
    "random.seed(init_seed)\n",
    "\n",
    "slip_prob = 0.3\n",
    "demo_horizon = 10\n",
    "num_demos = 1\n",
    "\n",
    "###Bayesian IRL\n",
    "beta = 10.0\n",
    "step_stdev = 0.2\n",
    "burn = 500\n",
    "skip = 5\n",
    "num_samples = 2000\n",
    "mcmc_norm = \"l2\"\n",
    "likelihood = \"birl\"\n",
    "\n",
    "mdp_env = mdp_worlds.lava_ambiguous_corridor()\n",
    "opt_sa = mdp.solve_mdp_lp(mdp_env)\n",
    "\n",
    "\n",
    "print(\"Cliff world\")\n",
    "print(\"Optimal Policy\")\n",
    "utils.print_policy_from_occupancies(opt_sa, mdp_env)\n",
    "print(\"reward\")\n",
    "utils.print_as_grid(mdp_env.r_s, mdp_env)\n",
    "print(\"features\")\n",
    "utils.display_onehot_state_features(mdp_env)\n",
    "\n",
    "init_demo_state = 1#mdp_env.num_cols * (mdp_env.num_rows - 1)\n",
    "traj_demonstrations = []\n",
    "demo_set = set()\n",
    "for d in range(num_demos):\n",
    "    # np.random.seed(init_seed + d)\n",
    "    # random.seed(init_seed + d)\n",
    "    s = init_demo_state #mdp_env.init_states[0] # only one initial state\n",
    "    demo = utils.rollout_from_usa(s, demo_horizon, opt_sa, mdp_env)\n",
    "    print(\"demo\", d, demo)\n",
    "    traj_demonstrations.append(demo)\n",
    "    for s_a in demo:\n",
    "        demo_set.add(s_a)\n",
    "demonstrations = list(demo_set)\n",
    "print(\"demonstration\")\n",
    "print(demonstrations)\n",
    "\n",
    "state_feature_list = [tuple(fs) for fs in mdp_env.state_features]\n",
    "pg.get_policy_string_from_trajectory(traj_demonstrations[0], state_feature_list, mdp_env, filename=\"./figs/lava_corridor/demo_lava_world.pdf\")\n",
    "\n",
    "\n",
    "#Now let's run Bayesian IRL on this demo in this mdp with a placeholder feature to see what happens.\n",
    "\n",
    "\n",
    "birl = bayesian_irl.BayesianIRL(mdp_env, beta, step_stdev, debug=False, mcmc_norm=mcmc_norm, likelihood=likelihood)\n",
    "\n",
    "map_w, map_u, r_chain, u_chain = birl.sample_posterior(demonstrations, num_samples, True)\n",
    "print(r_chain)\n",
    "import matplotlib.pyplot as plt\n",
    "# plt.figure()\n",
    "# for w in range(len(r_chain[0])):\n",
    "#     plt.plot(r_chain[:,w],label=\"feature {}\".format(w))\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "r_chain_burned = r_chain[burn::skip]\n",
    "\n",
    "u_expert = utils.u_sa_from_demos(traj_demonstrations, mdp_env)\n",
    "expert_returns = np.sort(np.dot(r_chain_burned, u_expert))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#get the r_sa matrix from the posterior \n",
    "Rsa = utils.convert_w_to_rsa(r_chain_burned, mdp_env)\n",
    "\n",
    "\n",
    "print(\"MAP\")\n",
    "print(\"map_weights\", map_w)\n",
    "map_r = np.dot(mdp_env.state_features, map_w)\n",
    "print(\"map reward\")\n",
    "utils.print_as_grid(map_r, mdp_env)\n",
    "print(\"Map policy\")\n",
    "utils.print_policy_from_occupancies(map_u, mdp_env)\n",
    "map_returns = np.sort(np.dot(Rsa, map_u))\n",
    "\n",
    "\n",
    "print(\"MEAN\")\n",
    "mean_w = np.mean(r_chain_burned, axis=0)\n",
    "print(\"mean_weights\", mean_w)\n",
    "mean_r = np.dot(mdp_env.state_features, mean_w)\n",
    "mean_r_sa = mdp_env.transform_to_R_sa(mean_w)\n",
    "mean_u_sa = mdp.solve_mdp_lp(mdp_env, reward_sa=mean_r_sa) #use optional argument to replace standard rewards with sample\n",
    "print('mean reward')\n",
    "utils.print_as_grid(mean_r, mdp_env)\n",
    "print(\"mean policy\")\n",
    "utils.print_policy_from_occupancies(mean_u_sa, mdp_env)\n",
    "\n",
    "pi = utils.get_policy_string_from_occupancies(mean_u_sa, mdp_env)\n",
    "state_feature_list = [tuple(fs) for fs in mdp_env.state_features]\n",
    "pg.plot_optimal_policy_lists(pi, state_feature_list, mdp_env.num_rows, mdp_env.num_cols, \"./figs/lava_corridor/mean.pdf\")\n",
    "    \n",
    "\n",
    "mean_returns = np.sort(np.dot(Rsa, mean_u_sa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let's see what CVaR optimization does.\n",
    "lambdas = [0., 0.1, 0.3, 0.5, 0.8, 1.0]\n",
    "alpha = 0.95\n",
    "for lamda in lambdas:\n",
    "    print()\n",
    "    print(\"ALPHA\", alpha, \"LAMBDA\", lamda)\n",
    "\n",
    "    debug = False\n",
    "    \n",
    "\n",
    "    n = r_chain_burned.shape[0]\n",
    "    posterior_probs = np.ones(n) / n  #uniform dist since samples from MCMC\n",
    "\n",
    "    \n",
    "    print(\"------ Robust Solution ---------\")\n",
    "    u_expert = np.zeros(mdp_env.num_actions * mdp_env.num_states)\n",
    "    robust_opt_usa, cvar_value, exp_ret = mdp.solve_max_cvar_policy(mdp_env, u_expert, r_chain_burned.transpose(), posterior_probs, alpha, debug, lamda)\n",
    "    #utils.print_stochastic_policy_action_probs(cvar_opt_usa, mdp_env_A)\n",
    "    print(\"Policy for lambda={} and alpha={}\".format(lamda, alpha))\n",
    "    utils.print_policy_from_occupancies(robust_opt_usa, mdp_env)\n",
    "\n",
    "#     robust_returns = np.sort(np.dot(Rsa, robust_opt_usa))\n",
    "\n",
    "\n",
    "    pi = utils.get_policy_string_from_occupancies(robust_opt_usa, mdp_env)\n",
    "    state_feature_list = [tuple(fs) for fs in mdp_env.state_features]\n",
    "    pg.plot_optimal_policy_lists(pi, state_feature_list, mdp_env.num_rows, mdp_env.num_cols, \"./figs/lava_corridor/robust_alpha\" + str(alpha) + \"lambda\" + str(lamda) + \".pdf\")\n",
    "    \n",
    "    \n",
    "\n",
    "    print(\"------ Regret Solution ---------\")\n",
    "    traj_demonstrations = [demonstrations]\n",
    "    u_expert = utils.u_sa_from_demos(traj_demonstrations, mdp_env)\n",
    "    \n",
    "    regret_opt_usa, cvar_value, exp_ret = mdp.solve_max_cvar_policy(mdp_env, u_expert, r_chain_burned.transpose(), posterior_probs, alpha, debug, lamda)\n",
    "    #utils.print_stochastic_policy_action_probs(cvar_opt_usa, mdp_env_A)\n",
    "    print(\"Policy for lambda={} and alpha={}\".format(lamda, alpha))\n",
    "    utils.print_policy_from_occupancies(regret_opt_usa, mdp_env)\n",
    "\n",
    "#     regret_returns = np.sort(np.dot(Rsa, regret_opt_usa))\n",
    "\n",
    "    pi = utils.get_policy_string_from_occupancies(regret_opt_usa, mdp_env)\n",
    "    state_feature_list = [tuple(fs) for fs in mdp_env.state_features]\n",
    "    pg.plot_optimal_policy_lists(pi, state_feature_list, mdp_env.num_rows, mdp_env.num_cols, \"./figs/lava_corridor/regret_alpha\" + str(alpha) + \"lambda\" + str(lamda) + \".pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LPAL solution\n",
    "u_expert = utils.u_sa_from_demos(traj_demonstrations, mdp_env)\n",
    "lpal_usa = mdp.solve_lpal_policy(mdp_env, u_expert)\n",
    "#utils.print_stochastic_policy_action_probs(cvar_opt_usa, mdp_env_A)\n",
    "print(\"lpal policy\")\n",
    "utils.print_policy_from_occupancies(lpal_usa, mdp_env)\n",
    "utils.print_stochastic_policy_action_probs(lpal_usa, mdp_env)\n",
    "pi_dict = utils.get_stoch_policy_string_dictionary_from_occupancies(lpal_usa, mdp_env)\n",
    "state_feature_list = [tuple(fs) for fs in mdp_env.state_features]\n",
    "pg.plot_optimal_policy_stochastic(pi_dict, state_feature_list, mdp_env.num_rows, mdp_env.num_cols, \"./figs/lava_corridor/lpal.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import maxent\n",
    "#just keep states in traj_demos\n",
    "maxent_demos = []\n",
    "for d in traj_demonstrations:\n",
    "    #add only states to demos\n",
    "    demo = []\n",
    "    for s,a in d:\n",
    "        demo.append(s)\n",
    "    maxent_demos.append(demo)\n",
    "maxent_usa, r_weights, maxent_pi = maxent.calc_max_ent_u_sa(mdp_env, maxent_demos)\n",
    "print(\"max ent policy\")\n",
    "utils.print_policy_from_occupancies(maxent_usa, mdp_env)\n",
    "utils.print_stochastic_policy_action_probs(maxent_usa, mdp_env)\n",
    "pi_dict = utils.get_stoch_policy_string_dictionary_from_occupancies(maxent_usa, mdp_env)\n",
    "state_feature_list = [tuple(fs) for fs in mdp_env.state_features]\n",
    "pg.plot_optimal_policy_stochastic(pi_dict, state_feature_list, mdp_env.num_rows, mdp_env.num_cols, \"./figs/lava_corridor/maxent.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let's see what CVaR optimization does.\n",
    "lambdas = [0.]\n",
    "alpha = 0.95\n",
    "for lamda in lambdas:\n",
    "    print()\n",
    "    print(\"ALPHA\", alpha, \"LAMBDA\", lamda)\n",
    "\n",
    "    debug = False\n",
    "    \n",
    "\n",
    "    n = r_chain_burned.shape[0]\n",
    "    posterior_probs = np.ones(n) / n  #uniform dist since samples from MCMC\n",
    "\n",
    "    \n",
    "    print(\"------ Robust Solution ---------\")\n",
    "    u_expert = np.zeros(mdp_env.num_actions * mdp_env.num_states)\n",
    "    robust_opt_usa, cvar_value, exp_ret = mdp.solve_max_cvar_policy(mdp_env, u_expert, r_chain_burned.transpose(), posterior_probs, alpha, debug, lamda)\n",
    "    #utils.print_stochastic_policy_action_probs(cvar_opt_usa, mdp_env_A)\n",
    "    print(\"Policy for lambda={} and alpha={}\".format(lamda, alpha))\n",
    "    utils.print_policy_from_occupancies(robust_opt_usa, mdp_env)\n",
    "\n",
    "    robust_returns = np.sort(np.dot(Rsa, robust_opt_usa))\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    print(\"------ Regret Solution ---------\")\n",
    "    traj_demonstrations = [demonstrations]\n",
    "    u_expert = utils.u_sa_from_demos(traj_demonstrations, mdp_env)\n",
    "    \n",
    "    regret_opt_usa, cvar_value, exp_ret = mdp.solve_max_cvar_policy(mdp_env, u_expert, r_chain_burned.transpose(), posterior_probs, alpha, debug, lamda)\n",
    "    #utils.print_stochastic_policy_action_probs(cvar_opt_usa, mdp_env_A)\n",
    "    print(\"Policy for lambda={} and alpha={}\".format(lamda, alpha))\n",
    "    utils.print_policy_from_occupancies(regret_opt_usa, mdp_env)\n",
    "\n",
    "    regret_returns = np.sort(np.dot(Rsa, regret_opt_usa))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lpal_returns = np.sort(np.dot(Rsa, lpal_usa))\n",
    "maxent_returns = np.sort(np.dot(Rsa, maxent_usa))\n",
    "mean_returns = np.sort(np.dot(Rsa, mean_u_sa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(20)\n",
    "plt.plot(expert_returns, ':',label='demo',linewidth=3)\n",
    "plt.plot(robust_returns,'--', label='Robust',linewidth=3)\n",
    "plt.plot(regret_returns, '-.',label='Regret',linewidth=3)\n",
    "plt.plot(lpal_returns,'-.',label=\"LPAL\", linewidth=3)\n",
    "plt.plot(maxent_returns,'-',label=\"MaxEnt\", linewidth=3)\n",
    "plt.legend(fontsize=18)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.ylabel(\"Return\", fontsize=18)\n",
    "plt.xlabel(\"Sorted Reward Function Samples\", fontsize=18)\n",
    "# plt.show()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"./figs/lava_corridor/robust_vs_baseline.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the baseline regret lines with respect to expert\n",
    "expert_returns = np.dot(r_chain_burned, u_expert)\n",
    "\n",
    "robust_returns_expert = np.sort(np.dot(Rsa, robust_opt_usa) - expert_returns)\n",
    "regret_returns_expert = np.sort(np.dot(Rsa, regret_opt_usa) - expert_returns)\n",
    "maxent_returns_expert = np.sort(np.dot(Rsa, maxent_usa) - expert_returns)\n",
    "lpal_returns_expert = np.sort(np.dot(Rsa, lpal_usa) - expert_returns)\n",
    "\n",
    "\n",
    "plt.figure(21)\n",
    "plt.plot(expert_returns-expert_returns, ':',label='demo',linewidth=3)\n",
    "# plt.plot(mean_returns,'-', label='mean',linewidth=2)\n",
    "#plt.plot(map_returns, label='MAP')\n",
    "plt.plot(robust_returns_expert,'--', label='Robust',linewidth=3)\n",
    "plt.plot(regret_returns_expert, '-.',label='Regret',linewidth=3)\n",
    "#plt.plot(mean_returns,':',label=\"BIRL\", linewidth=3)\n",
    "plt.plot(lpal_returns_expert,'-.',label=\"LPAL\", linewidth=3)\n",
    "plt.plot(maxent_returns_expert,'-',label=\"MaxEnt\", linewidth=3)\n",
    "#plt.legend(fontsize=18)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.ylabel(\"Return\", fontsize=18)\n",
    "plt.xlabel(\"Sorted Reward Function Samples\", fontsize=18)\n",
    "# plt.show()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"./figs/lava_corridor/robust_vs_baseline_regretnormalized.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import generate_efficient_frontier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's try and plot the efficient frontier for the baseline regret policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_range = [0., 0.1,0.2, 0.3,0.4, 0.5,0.6,0.7, 0.8,0.9,0.95,0.99]\n",
    "u_expert = utils.u_sa_from_demos(traj_demonstrations, mdp_env)\n",
    "\n",
    "#generate_efficient_frontier.calc_frontier(mdp_env, u_expert, posterior, posterior_probs, lambda_range, alpha, debug=False)\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "#for alpha in alpha_range:\n",
    "cvar_rets = generate_efficient_frontier.calc_frontier(mdp_env, u_expert, r_chain_burned.transpose(), posterior_probs, lambda_range, alpha, debug=False)\n",
    "\n",
    "cvar_rets_array = np.array(cvar_rets)\n",
    "print(cvar_rets_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#robust version\n",
    "u_expert = utils.u_sa_from_demos(traj_demonstrations, mdp_env)        \n",
    "#input()\n",
    "plt.figure()\n",
    "ax = plt.subplot(111)\n",
    "#plt.title(r\"$\\alpha = {}$\".format(alpha))\n",
    "ax.plot(cvar_rets_array[:,0], cvar_rets_array[:,1], '-o',label=\"BROIL\",color='green')\n",
    "#go through and label the points in the figure with the corresponding lambda values\n",
    "unique_pts_lambdas = []\n",
    "unique_pts = []\n",
    "for i,pt in enumerate(cvar_rets_array):\n",
    "    unique = True\n",
    "    for upt in unique_pts:\n",
    "        if np.linalg.norm(upt - pt) < 0.00001:\n",
    "            unique = False\n",
    "            break\n",
    "    if unique:\n",
    "        unique_pts_lambdas.append((pt[0], pt[1], lambda_range[i]))\n",
    "        unique_pts.append(np.array(pt))\n",
    "#calculate offset\n",
    "offsetx = (np.max(cvar_rets_array[:,0]) - np.min(cvar_rets_array[:,0]))/30\n",
    "offsety = (np.max(cvar_rets_array[:,1]) - np.min(cvar_rets_array[:,1]))/17\n",
    "#print(offsetx)\n",
    "#input()\n",
    "for i,pt in enumerate(unique_pts_lambdas):\n",
    "    if i in [0]:\n",
    "        ax.text(pt[0]- 9*offsetx, pt[1] - 10*offsety, r\"$\\lambda \\in [{},{})$\".format(str(pt[2]), unique_pts_lambdas[i+1][2]), fontsize=19,  fontweight='bold')\n",
    "    elif i in [1]:\n",
    "        ax.text(pt[0] + 5.5*offsetx, pt[1] - 4*offsety , r\"$\\lambda \\in [{},{})$\".format(str(pt[2]), unique_pts_lambdas[i+1][2]), fontsize=19,  fontweight='bold')\n",
    "    elif i in [2]:\n",
    "        ax.text(pt[0] + 5*offsetx,pt[1] + 1*offsety  , r\"$\\lambda \\in [{},{})$\".format(str(pt[2]), unique_pts_lambdas[i+1][2]), fontsize=19,  fontweight='bold')\n",
    "    elif i in [3]:\n",
    "        ax.text(pt[0]- 128*offsetx, pt[1] - 3*offsety, r\"$\\lambda \\in [{},1.0]$\".format(str(pt[2]),1), fontsize=19,  fontweight='bold')\n",
    "    else:\n",
    "        ax.text(pt[0]+2*offsetx, pt[1] + 1.5*offsety, r\"$\\lambda = {}$\".format(str(pt[2])), fontsize=19,  fontweight='bold')\n",
    "#plt.axis([-1.43, -1.25, -1.05, -0.99])\n",
    "\n",
    "#plot Maxent and LPAL\n",
    "\n",
    "cvar_maxent, expret_maxent = mdp.solve_cvar_expret_fixed_policy(mdp_env, maxent_usa, u_expert, r_chain_burned.transpose(), posterior_probs, alpha, debug=False)\n",
    "ax.plot(cvar_maxent, expret_maxent, 'o',label='MaxEnt', color='purple')\n",
    "\n",
    "#plot LPAL\n",
    "cvar_lpal, expret_lpal = mdp.solve_cvar_expret_fixed_policy(mdp_env, lpal_usa, u_expert, r_chain_burned.transpose(), posterior_probs, alpha, debug=False)\n",
    "ax.plot(cvar_lpal, expret_lpal, 'o',label='LPAL', color='red')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.xticks(fontsize=18) \n",
    "plt.yticks(fontsize=18) \n",
    "plt.xlabel(\"Robustness (CVaR)\", fontsize=22)\n",
    "plt.ylabel(\"Expected Return\", fontsize=22)\n",
    "\n",
    "plt.legend(loc='best', fontsize=20)\n",
    "plt.tight_layout()\n",
    "#plt.show()\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "plt.savefig(\"./figs/lava_corridor/efficient_frontier.pdf\")\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvarirl",
   "language": "python",
   "name": "cvarirl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
